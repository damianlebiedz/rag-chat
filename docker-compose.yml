services:

  llm:
    image: ollama/ollama:0.12.9
    command: serve
    ports:
      - "11434:11434"
    networks:
      - net
    volumes:
      - ./.ollama:/root/.ollama

  pull-model:
    image: rag-chat/pull-model:latest
    build:
      context: .
      dockerfile: containers/pull_model.Dockerfile
    environment:
      - OLLAMA_BASE_URL=${OLLAMA_HOST}:${OLLAMA_PORT}
      - LLM
    depends_on:
      - llm
    networks:
      - net
    tty: true

  database:
    image: neo4j:5.26
    ports:
      - "7687:7687"
      - "7474:7474"
    volumes:
      - ./data:/data
    environment:
      - NEO4J_AUTH=${NEO4J_USERNAME}/${NEO4J_PASSWORD}
      - NEO4J_PLUGINS=["apoc"]
      - NEO4J_db_tx__log_rotation_retention__policy=false
      - NEO4J_dbms_security_procedures_unrestricted=apoc.*
    healthcheck:
      test: [ "CMD-SHELL", "wget --no-verbose --tries=1 --spider localhost:7474 || exit 1" ]
      interval: 10s
      timeout: 10s
      retries: 5
    networks:
      - net

  app:
    image: rag-chat/base:latest
    build:
      context: .
      dockerfile: containers/base.Dockerfile
    env_file: .env
    environment:
      - DOCKER_ENV=true
    volumes:
      - ./embedding_model:/app/embedding_model
    networks:
      - net
    depends_on:
      database:
        condition: service_healthy
      pull-model:
        condition: service_completed_successfully
    ports:
      - "8503:8503"
    command: streamlit run app.py --server.port=8503 --server.address=0.0.0.0
    healthcheck:
      test: [ "CMD", "curl", "--fail", "http://localhost:8503/_stcore/health" ]
      interval: 30s
      timeout: 10s
      retries: 3

  api:
    image: rag-chat/base:latest
    env_file: .env
    environment:
      - DOCKER_ENV=true
    networks:
      - net
    depends_on:
      database:
        condition: service_healthy
      pull-model:
        condition: service_completed_successfully
    ports:
      - "8504:8504"
    command: uvicorn api.api:app --host 0.0.0.0 --port 8504
    healthcheck:
      test: [ "CMD", "curl", "--fail", "http://localhost:8504" ]
      interval: 30s
      timeout: 10s
      retries: 3

  front-end:
    image: rag-chat/front-end:latest
    build:
      context: .
      dockerfile: containers/front-end.Dockerfile
    x-develop:
      watch:
        - action: sync
          path: ./front-end
          target: /app
          ignore:
            - ./front-end/node_modules/
        - action: rebuild
          path: ./front-end/package.json
    depends_on:
      api:
        condition: service_healthy
    networks:
      - net
    ports:
      - "8505:8505"

networks:
  net:

volumes:
  ollama:
